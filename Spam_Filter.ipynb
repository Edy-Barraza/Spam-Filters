{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Spam Filter with a Soft Margin Support Vector Machine, Ridge Regression, and Logistic Regression.</h2>\n",
    "\n",
    "<p>\n",
    "Building a spam filter using a bag of words approach is a binary classification problem in a high dimensional feature space. This means data is likely linearly seperable, and if it is not then there is a seperating hyperplane that will make predictions with high accuracy. To find this seperating hyperlane, we can minimize the following loss functions using the adagrad gradient descent algorithm:\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loss Functions</h3>\n",
    "\n",
    "<ol>\n",
    "    <li> Ridge Regression: ($y_i\\in\\{+1,-1\\}$); ${\\cal L}(w)= \\frac{1}{n}\\sum_{i=1}^n (w^\\top x_i-y_i)^2+\\lambda \\|w\\|_2^2$ \n",
    "    </li>\n",
    "    <li> Hinge Loss (SVM): ($y_i\\in\\{+1,-1\\}$);  ${\\cal L}(w)=\\sum_{i=1}^n \\max \\left(1-y_i(w^\\top x_i+b),0\\right)+\\lambda \\|w\\|_2^2$ </li>\n",
    "    <li> Logistic Regression: ($y_i\\in\\{+1,-1\\}$);  ${\\cal L}(w)=\\sum_{i=1}^n \\log(1+\\exp{(-y_i w^\\top x_i)})$ </li>\n",
    "    \n",
    "</ol>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Feature Extraction</h3>\n",
    "<p>We can take a bag of words approach, hashing each word to the index of an array and adding a count of one if the word is present </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('PDF')\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5535 input emails.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5535, 6000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the email and hashes the symbols into a vector\n",
    "def extractfeaturesnaive(path, B):\n",
    "    with open(path, 'r') as femail:\n",
    "        # initialize all-zeros feature vector\n",
    "        v = np.zeros(B)\n",
    "        email = femail.read()\n",
    "        # breaks for non-ascii characters\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            v[hash(token) % B] = 1\n",
    "    return v\n",
    "\n",
    "def loadspamdata(extractfeatures, B=6000, path=\"SMSSpamCollection\"):\n",
    "    '''\n",
    "    INPUT:\n",
    "    extractfeatures : function to extract features\n",
    "    B               : dimensionality of feature space\n",
    "    path            : the path of folder to be processed\n",
    "    \n",
    "    OUTPUT:\n",
    "    X, Y\n",
    "    '''\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        allemails = [x for x in f.read().split('\\n') if ' ' in x]\n",
    "    \n",
    "    xs = np.zeros((len(allemails), B))\n",
    "    ys = np.zeros(len(allemails))\n",
    "    for i, line in enumerate(allemails):\n",
    "        label, filename = line.split('\t')\n",
    "        \n",
    "        # make labels +1 for \"spam\" and -1 for \"ham\"\n",
    "        ys[i] = (label == 'spam') * 2 - 1\n",
    "        xs[i, :] = extractfeatures(path, B)\n",
    "    print('Loaded %d input emails.' % len(ys))\n",
    "    return xs, ys\n",
    "\n",
    "X,Y = loadspamdata(extractfeaturesnaive)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation\n",
    "n, d = X.shape\n",
    "cutoff = int(np.ceil(0.8 * n))\n",
    "# indices of training samples\n",
    "xTr = X[:cutoff,:]\n",
    "yTr = Y[:cutoff]\n",
    "# indices of testing samples\n",
    "xTv = X[cutoff:,:]\n",
    "yTv = Y[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ridge Regression </h3>\n",
    "\n",
    "<p> ${\\cal L}(w)=\\frac{1}{n}\\sum_{i=1}^n (w^\\top x_i-y_i)^2+\\lambda \\|w\\|_2^2$; ($y_i\\in\\{+1,-1\\}$)  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge(w,xTr,yTr,lmbda):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    lmbda : regression constant (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    #loss\n",
    "    #dotted = np.dot(xTr,w)\n",
    "    #subbed = dotted - yTr\n",
    "    #sqrd   = np.square(subbed)\n",
    "    #summed = np.sum(sqrd)/n\n",
    "    #dub    = lmbda*np.dot(w,w)\n",
    "    \n",
    "    #gradient\n",
    "    dotted = np.dot(xTr,w)\n",
    "    subbed = dotted - yTr\n",
    "    summed = 2*(np.dot(subbed.T,xTr)/n + lmbda*w)\n",
    "    \n",
    "\n",
    "    return (lmbda*np.dot(w,w)+ np.sum(np.square(np.dot(xTr,w)-yTr ))/n), summed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Gradient Verification </h3>\n",
    "\n",
    "<p>An  alternative to  deriving the gradient analytically is to estimate it numerically. This is very slow, so it is only used to check for correctness. The function also plots an image of the gradient values (blue) and their estimates (red). If everything is correct, these two should be right on top of each other. The norm difference between the numerical and analytical solution should be very small (smaller than $10^{-8}$). </p>\n",
    "\n",
    "<p>\n",
    "NOTE: This verification is only valid if the loss function was implemented correctly, since the gradient corresponds to the loss. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm ratio is 0.0000000102.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAF3CAYAAAAcmcfdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuEmSJBWEiZskSVJB/D/vgzrsMBd9wwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10deba4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def numericalgradient(fun,w,e):\n",
    "    # get dimensionality\n",
    "    d = len(w)\n",
    "    # initialize numerical derivative\n",
    "    dh = np.zeros(d)\n",
    "    # go through dimensions\n",
    "    for i in range(d):\n",
    "        # copy the weight vector\n",
    "        nw = w.copy()\n",
    "        # perturb dimension i\n",
    "        nw[i] += e\n",
    "        # compute loss\n",
    "        l1, temp = fun(nw)\n",
    "        # perturb dimension i again\n",
    "        nw[i] -= 2*e\n",
    "        # compute loss\n",
    "        l2, temp = fun(nw)\n",
    "        # the gradient is the slope of the loss\n",
    "        dh[i] = (l1 - l2) / (2*e)\n",
    "    return dh\n",
    "\n",
    "def checkgrad(fun,w,e):\n",
    "    # evaluate symbolic gradient from fun()\n",
    "    loss,dy = fun(w)\n",
    "    # estimate gradient numerically from fun()\n",
    "    dh = numericalgradient(fun,w,e)\n",
    "    \n",
    "    # ii = dy.argsort()\n",
    "    ii = np.array([i for i in range(len(dy))])\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter([i for i in range(len(dy))], dh[ii], c='b', marker='o', s=60)\n",
    "    plt.scatter([i for i in range(len(dy))], dy[ii], c='r', marker='.', s=50)\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Gradient value')\n",
    "    plt.legend([\"numeric\",\"symbolic\"])\n",
    "    \n",
    "    # return the norm of the difference scaled by the norm of the sum\n",
    "    return np.linalg.norm(dh - dy) / np.linalg.norm(dh + dy)\n",
    "\n",
    "# set lmbda (Î») arbitrarily\n",
    "lmbda = 0.1\n",
    "# dimensionality of the input\n",
    "_, d = xTr.shape\n",
    "# evaluate loss on random vector\n",
    "w = np.random.rand(d)\n",
    "\n",
    "ratio = checkgrad(lambda weight: ridge(weight,xTr,yTr,lmbda),w,1e-05)\n",
    "print(\"The norm ratio is %.10f.\" % ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adagrad Gradient Descent </h3>\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adagrad(func,w,alpha,maxiter,eps,delta=1e-02):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    func    : function to minimize\n",
    "              (loss, gradient = func(w))\n",
    "    w       : d dimensional initial weight vector \n",
    "    alpha   : initial gradient descent stepsize (scalar)\n",
    "    maxiter : maximum amount of iterations (scalar)\n",
    "    eps     : epsilon value\n",
    "    delta   : if norm(gradient)<delta, it quits (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "     \n",
    "    w      : d dimensional final weight vector\n",
    "    losses : vector containing loss at each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    #get subarray if we terminate early \n",
    "    losses = np.zeros(maxiter)\n",
    "    zd     = np.zeros(w.size)\n",
    "\n",
    "    for i in range(0,maxiter):\n",
    "        loss,grad = func(w)\n",
    "        losses[i]=loss\n",
    "        zd = zd + np.square(grad)\n",
    "        w  = w  - alpha*grad/(np.sqrt(zd+eps))\n",
    "        if (np.linalg.norm(grad)<delta ):\n",
    "            return w,losses[:i+1]\n",
    "        \n",
    "        \n",
    "            \n",
    "    return w,losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adagrad Convergence Visual</h3>\n",
    "<p>We can visualize adagrad's path to convergence to ensure that the algorithm is behaving as expected </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 45.601493\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAGDCAYAAABuj7cYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10de97cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, d = xTr.shape\n",
    "eps = 1e-06\n",
    "w, losses = adagrad(lambda weight: ridge(weight, xTr, yTr, lmbda), np.random.rand(d), 1, 1000, eps)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogy(losses, c='r', linestyle='-')\n",
    "plt.xlabel(\"gradient updates\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Adagrad convergence\")\n",
    "print(\"Final loss: %f\" % losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Final Evaluation </h3>\n",
    "\n",
    "<p>After this we can check training and validation accuracy by running the cell below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 86.40%\n",
      "Validation accuracy 86.90%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def class_unbiased(xi,w):\n",
    "    val=np.dot(xi,w)\n",
    "    if val>0:\n",
    "        return 1\n",
    "    elif val<0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def linclassify(w,xTr):\n",
    "    ## fill in your code here\n",
    "    w = w.reshape(-1)\n",
    "    ## fill in code here\n",
    "    return np.fromiter((class_unbiased(xi,w) for xi in xTr),int, count=xTr.shape[0])\n",
    "    \n",
    "    \n",
    "\n",
    "# evaluate training accuracy\n",
    "preds = linclassify(w,xTr)\n",
    "trainingacc = np.mean(preds==yTr)\n",
    "# evaluate testing accuracy\n",
    "preds = linclassify(w,xTv)\n",
    "validationacc = np.mean(preds==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The same follows for our other two other loss functions, <code>hinge</code> and <code>logistic</code>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hinge Loss</h3>\n",
    "<p> ${\\cal L}(w)=\\sum_{i=1}^n \\max \\left(1-y_i(w^\\top x_i+b),0\\right)+\\lambda \\|w\\|_2^2$ ; ($y_i\\in\\{+1,-1\\}$)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge(w,xTr,yTr,lmbda):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    lmbda : regression constant (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    n, d       = xTr.shape\n",
    "    \n",
    "    regulizer  = lmbda*np.dot(w,w)\n",
    "    pre_summed = np.maximum(1-yTr*np.dot(xTr,w),0)\n",
    "    \n",
    "    loss       = np.sum(pre_summed) + regulizer \n",
    "    reg_grad   = 2*lmbda*w\n",
    "    \n",
    "    \n",
    "    the_bool = yTr*np.dot(xTr,w)\n",
    "    xTr_copy = np.copy(xTr)\n",
    "    \n",
    "    xTr_copy[the_bool>=1]=0\n",
    "    \n",
    "    grad = np.dot(-yTr,xTr_copy)+ reg_grad\n",
    "    \n",
    "\n",
    "    return loss,grad\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hinge Loss Verification </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient sanity check\n",
    "lmbda = 0.1\n",
    "_, d = xTr.shape\n",
    "w = np.random.rand(d)\n",
    "ratio = checkgrad(lambda weight: hinge(weight,xTr,yTr,lmbda),w,1e-05)\n",
    "print(\"The norm ratio is %.10f.\" % ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 86.40%\n",
      "Validation accuracy 86.90%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lmbda = 0.1\n",
    "w, losses = adagrad(lambda weight: hinge(weight,xTr,yTr,lmbda), np.random.rand(d), 1, 1000, 1e-06)\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds = linclassify(w,xTr)\n",
    "trainingacc = np.mean(preds==yTr)\n",
    "# evaluate testing accuracy\n",
    "preds = linclassify(w,xTv)\n",
    "validationacc = np.mean(preds==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Logistic Regression:  </h3>\n",
    "<p> ${\\cal L}(w)=\\sum_{i=1}^n \\log(1+\\exp{(-y_i w^\\top x_i)})$ ; ($y_i\\in\\{+1,-1\\}$)  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(w,xTr,yTr):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    # w_dot_xi = np.dot(xTr,w)\n",
    "    e     = np.exp(np.dot(xTr,w) * -yTr)\n",
    "    #loss  = np.sum(np.log(1+e))\n",
    "    loss   = np.sum(np.log(1 + np.exp(np.dot(xTr,w) * -yTr)))\n",
    "\n",
    "    pre = -yTr*e/(1+e)\n",
    "    grad = np.dot(xTr.T,pre)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return loss, grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logistic Regression Verification</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient sanity check\n",
    "_, d = xTr.shape\n",
    "w = np.random.rand(d)\n",
    "ratio = checkgrad(lambda weight: logistic(weight,xTr,yTr),w,1e-05)\n",
    "print(\"The norm ratio is %.10f.\" % ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, losses = adagrad(lambda weight: logistic(weight, xTr, yTr), np.random.rand(d), 1, 1000, 1e-06)\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds = linclassify(w,xTr)\n",
    "trainingacc = np.mean(preds==yTr)\n",
    "# evaluate testing accuracy\n",
    "preds = linclassify(w,xTv)\n",
    "validationacc = np.mean(preds==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
